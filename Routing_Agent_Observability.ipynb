{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Observability/blob/main/Routing_Agent_Observability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL0ehB688W7m"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnJM1Js08cA6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pooXnxdK2YDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIjCyWxu8eTZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"]=\"agent_observability\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=userdata.get('OPENAI_API_KEY')\n",
        "LANGSMITH_TRACING=True\n",
        "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "LANGSMITH_API_KEY=userdata.get('LANGCHAIN_API_KEY')\n",
        "LANGSMITH_PROJECT=\"agent_observability\"\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P4JFvnnIiqb"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrCN4qNm8gPH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp0wbYoD8j_L"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkpBWTSAwzi"
      },
      "outputs": [],
      "source": [
        "# In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  if not os.path.exists(f\"{PERSIST_INDEX_DIR}{index_name}/\"):\n",
        "    # Load the documents\n",
        "    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Store the index to disk\n",
        "    index.storage_context.persist(f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "  else: # Load index from disk\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGX9M8OnIxhN"
      },
      "outputs": [],
      "source": [
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "docs_OECD_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/OECD/\").load_data()\n",
        "# Load OECD guidelines documents for Form990\n",
        "docs_Form990_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/Form990/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvAM5eBOAyy4"
      },
      "outputs": [],
      "source": [
        "#initialise a storage context and use that for both Vector Index and Summary Index for OECD\n",
        "oecd_nodes = Settings.node_parser.get_nodes_from_documents(docs_OECD_guidelines)\n",
        "form990_nodes = Settings.node_parser.get_nodes_from_documents(docs_Form990_guidelines)\n",
        "\n",
        "oecd_storage_context = StorageContext.from_defaults()\n",
        "\n",
        "oecd_storage_context.docstore.add_documents(oecd_nodes)\n",
        "oecd_storage_context.docstore.add_documents(form990_nodes)\n",
        "# Setup Vector and Summary Index from Storage Context\n",
        "oecd_summary_index = SummaryIndex(oecd_nodes, storage_context=oecd_storage_context)\n",
        "oecd_vector_index = VectorStoreIndex(oecd_nodes, storage_context=oecd_storage_context)\n",
        "\n",
        "# Setup Indices.In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "OECD_index = get_index(\"OECDTPGuidelines\",f\"{data_dir}/RAG/data/OECD/OECD_Transfer_Pricing_Guidelines.pdf\")\n",
        "form990_guidelines_index = get_index(\"Form990Guidelines\",f\"{data_dir}/RAG/data/Form990/Form990_Guidelines.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0svi3grHC-hF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from langsmith import Client, traceable\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "\n",
        "OECD_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "# Create tools for the query engines and by applying @traceable to the tools, we gain observability into each tool's query operations.\n",
        "\n",
        "OECD_query_tool = QueryEngineTool(\n",
        "                      query_engine=OECD_engine, # Changed to OECD_engine\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"OECD_QueryEngineTool_2022\",\n",
        "                          description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "# Instead of functions, use the query engine objects directly\n",
        "Form990_query_tool = QueryEngineTool(\n",
        "                      query_engine=form990_guidelines_engine, # Changed to form990_guidelines_engine\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"form990_2022\",\n",
        "                          description=\"Provides information about Central Jersey Form990 filling for the year 2022\"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "tools = [OECD_query_tool, Form990_query_tool]\n",
        "\n",
        "filing_engine = RouterQueryEngine(\n",
        "                      selector= LLMSingleSelector.from_defaults(),\n",
        "                      query_engine_tools=tools\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjT0PRiNEPqN"
      },
      "outputs": [],
      "source": [
        "OECD_query=\"What documentation and approach is required for the valuation of assets in Transfer Pricing?\"\n",
        "Form990_query=\"An organization receives contributions worth of $15000 which Form within Form990 should the organization complete before filing returns to IRS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufPBLWPdFX2v"
      },
      "outputs": [],
      "source": [
        "response = filing_engine.query(OECD_query)\n",
        "print(\"\\n.--------OECD_query response----------.\\n\")\n",
        "print (response)\n",
        "response = filing_engine.query(Form990_query)\n",
        "print(\"\\n.--------Form990_query response----------.\\n\")\n",
        "print (response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOIWBQ48H4Tq"
      },
      "outputs": [],
      "source": [
        "# Define the Summary and Vector query engines for OECD\n",
        "summary_query_engine = oecd_summary_index.as_query_engine(response_mode= \"tree_summarize\")\n",
        "vector_query_engine = oecd_vector_index.as_query_engine()\n",
        "\n",
        "# Now Create the query engine tools from the above query engines\n",
        "summary_tool = QueryEngineTool(\n",
        "                query_engine=summary_query_engine,\n",
        "                metadata=ToolMetadata(\n",
        "                    name=\"OECD_Summary\",\n",
        "                    description=\"Summarizes the OECD guidelines for Transfer Pricing\"\n",
        "                    )\n",
        "                )\n",
        "vector_tool = QueryEngineTool(\n",
        "                query_engine=vector_query_engine,\n",
        "                metadata = ToolMetadata(\n",
        "                    name=\"OECD_Guidelines_QA\",\n",
        "                    description=\"Retrieves answers for questions on OECD\"\n",
        "                    )\n",
        "                )\n",
        "oecd_tools = [summary_tool, vector_tool]\n",
        "# Now define the Router Query Engine\n",
        "oecd_engine = RouterQueryEngine(\n",
        "                      selector= LLMSingleSelector.from_defaults(),\n",
        "                      query_engine_tools=oecd_tools\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'oecd_engine' are already defined from your previous code\n",
        "client = Client(api_key=LANGSMITH_API_KEY)\n",
        "\n",
        "#@traceable to the individual tools that the RouterQueryEngine uses is a very effective way\n",
        "#to gain granular visibility into which tool is selected and how that specific tool performs.\n",
        "@traceable(run_type=\"chain\", client=client, tracing_level=\"verbose\")\n",
        "def query_oecd_engine_summary(engine: RouterQueryEngine, query_text: str):\n",
        "  \"\"\"\n",
        "  A traceable wrapper function for querying the oecd_engine,\n",
        "  intended to route to the summary tool.\n",
        "  \"\"\"\n",
        "  return engine.query(query_text)\n",
        "\n",
        "@traceable(run_type=\"chain\", client=client, tracing_level=\"verbose\")\n",
        "def query_oecd_engine_vector(engine: RouterQueryEngine, query_text: str):\n",
        "  \"\"\"\n",
        "  A traceable wrapper function for querying the oecd_engine,\n",
        "  intended to route to the vector (QA) tool.\n",
        "  \"\"\"\n",
        "  return engine.query(query_text)"
      ],
      "metadata": {
        "cellView": "code",
        "id": "Ak6fNAIEcS6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use these wrapper functions to query the oecd_engine\n",
        "# Example using the summary query\n",
        "response = query_oecd_engine_summary(oecd_engine, \"Summarize the OECD guidelines for Transfer Pricing?\")\n",
        "print (response)\n",
        "# To Confirm whether the Summary Engine was used\n",
        "print (response.metadata[\"selector_result\"])\n",
        "\n",
        "# Example using the vector (QA) query\n",
        "response = query_oecd_engine_vector(oecd_engine, \"What would be the best possible way to evaluate the Intangibles in Transfer Pricing?\")\n",
        "print (response)\n",
        "# Confirm that Vector Engine was used.\n",
        "print (response.metadata[\"selector_result\"])"
      ],
      "metadata": {
        "id": "Yt-zfXsed1dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYTIob1bLS7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "response = oecd_engine.query(\"Summarize the OECD guidelines for Transfer Pricing?\")\n",
        "print (response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIgKtJ7xo09O"
      },
      "outputs": [],
      "source": [
        "# To Confirm whether the Summary Engine was used\n",
        "print (response.metadata[\"selector_result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mINCkWbpQvO"
      },
      "outputs": [],
      "source": [
        "# Now route to a Question Answer engine\n",
        "response = oecd_engine.query(\"What would be the best possible way to evaluate the Intangibles in Transfer Pricing?\")\n",
        "print (response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wETxPwaSqWJ2"
      },
      "outputs": [],
      "source": [
        "# Confirm that Vector Engine was used.\n",
        "print (response.metadata[\"selector_result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tET00dfrcr0"
      },
      "outputs": [],
      "source": [
        "!pip install -U faiss-cpu sentence_transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESbgPvWSrQ6-"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "client = Client(api_key=LANGSMITH_API_KEY)\n",
        "#FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
        "#IndexFlatL2 uses Euclidean distance to find the nearest neighbors.\n",
        "#The 768 indicates the dimension of the vectors that will be stored in the index.\n",
        "#This corresponds to the output dimension of the chosen sentence transformer model.\n",
        "class SemanticCaching:\n",
        "    def __init__(self, json_file='cacheagent.json'):\n",
        "        # Initialize Faiss index  with Euclidean distance\n",
        "        self.index =faiss.IndexFlatL2(768)  # Use IndexFlatL2 with Euclidean distance\n",
        "        if self.index.is_trained:\n",
        "            print('Index trained')\n",
        "\n",
        "        # Initialize Sentence Transformer\n",
        "        #SentenceTransformer model called 'all-mpnet-base-v2'. This model is used to convert text queries into numerical vector representations called embeddings.\n",
        "        #Semantic similarity between questions is determined by the distance between their embeddings.\n",
        "        self.encoder = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "\n",
        "        # Uncomment the following lines to use DialoGPT for question generation\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "        # self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "        # Set Euclidean distance threshold. This threshold is used to determine if a cached answer is sufficiently similar to the current query.\n",
        "        # If the Euclidean distance between the query embedding and a cached embedding is less than or equal to this threshold, the cached answer is considered a match.\n",
        "        self.euclidean_threshold = 0.3\n",
        "        self.json_file = json_file\n",
        "        self.load_cache()\n",
        "\n",
        "    def load_cache(self):\n",
        "        # Load cache from JSON file, creating an empty cache if the file is not found\n",
        "        try:\n",
        "            with open(self.json_file, 'r') as file:\n",
        "                self.cache = json.load(file)\n",
        "        except FileNotFoundError:\n",
        "            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n",
        "    def save_cache(self):\n",
        "        # Save the cache to the JSON file\n",
        "        with open(self.json_file, 'w') as file:\n",
        "            json.dump(self.cache, file)\n",
        "    @traceable(run_type=\"chain\", client=client, tracing_level=\"verbose\")\n",
        "    def ask(self, question: str) -> str:\n",
        "        # Method to retrieve an answer from the cache or generate a new one\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            l = [question]\n",
        "            embedding = self.encoder.encode(l)\n",
        "\n",
        "            # Search for the nearest neighbor in the index\n",
        "            D, I = self.index.search(embedding, 1)\n",
        "\n",
        "            if D[0] >= 0:\n",
        "                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
        "                    row_id = int(I[0][0])\n",
        "                    print(f'Found cache in row: {row_id} with score {1 - D[0][0]}') #score inversed to show similarity\n",
        "                    end_time = time.time()\n",
        "                    elapsed_time = end_time - start_time\n",
        "                    print(f\"Time taken: {elapsed_time} seconds\")\n",
        "                    return self.cache['response_text'][row_id]\n",
        "\n",
        "            # Handle the case when there are not enough results or Euclidean distance is not met\n",
        "            answer, response_text = self.generate_answer(question)\n",
        "\n",
        "            self.cache['questions'].append(question)\n",
        "            self.cache['embeddings'].append(embedding[0].tolist())\n",
        "            self.cache['answers'].append(answer)\n",
        "            self.cache['response_text'].append(response_text)\n",
        "\n",
        "            self.index.add(embedding)\n",
        "            self.save_cache()\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(f\"Time taken: {elapsed_time} seconds\")\n",
        "\n",
        "            return response_text\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n",
        "    @traceable(run_type=\"chain\", client=client, tracing_level=\"verbose\")\n",
        "    def generate_answer(self, question: str) -> str:\n",
        "        # Method to generate an answer using a separate function (make_prediction in this case)\n",
        "        try:\n",
        "            result = oecd_engine.query(question)\n",
        "            #response_text = result['data']['response_text']\n",
        "            response_text=result\n",
        "\n",
        "            return result, response_text\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zpmY0eMr0vc"
      },
      "outputs": [],
      "source": [
        "cache = SemanticCaching()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUYz0qqdtcEe"
      },
      "outputs": [],
      "source": [
        "import markdown\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def print_gpt_markdown(markdown_text):\n",
        "    display(Markdown(markdown_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2MQgAUesCz_"
      },
      "outputs": [],
      "source": [
        "# Now Cache the request and response using Semantic Caching wherein the question is routed to a Question Answer engine\n",
        "response = cache.ask(\"What would be the best possible way to evaluate the Intangibles in Transfer Pricing?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying the same agent with a different question but with same semantic meaning\n",
        "response = cache.ask(\"How can Intangibles be effectively evaluated in Transfer Pricing?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "zjYw1nLP0AcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying the same agent with a different question but with same semantic meaning\n",
        "response = cache.ask(\"What methods are recommended for assessing Intangibles in Transfer Pricing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2xi4s6fG0hiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying the same agent with a different question but with same semantic meaning\n",
        "response = cache.ask(\"What strategies are considered most effective for evaluating Intangibles in Transfer Pricing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "nnY-g_AL0x3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying the same agent with a different question but with same semantic meaning\n",
        "response = cache.ask(\"How should Intangibles be properly assessed in the context of Transfer Pricing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "KwSGBMx0032z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying the same agent with a different question but with same semantic meaning\n",
        "response = cache.ask(\"What are the most reliable approaches for evaluating Intangibles in Transfer Pricing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "jo1GpO0m08rY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0sgKWAhSGe7DFZL8bf1Hb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}